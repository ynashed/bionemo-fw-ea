apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  #generateName: training-
  namespace: runai-amplify
  name: pretrain-amplify-350m
spec:
  elasticPolicy:
    maxReplicas: 4
    maxRestarts: 10
    minReplicas: 4
    nProcPerNode: 8
    rdzvBackend: c10d
  pytorchReplicaSpecs:
    Worker:
      replicas: 4
      restartPolicy: OnFailure
      template:
        metadata:
          annotations:
            k8s.v1.cni.cncf.io/networks: ''
            sidecar.istio.io/inject: 'false'
        spec:
          containers:
          - command: ["bash", "-euxc"]
            args:
              - |
                export CURRENT_TIMESTAMP=$(date +%s) && \
                export REPLICA_LOG_DIR=/home/local/.cache/logs/replica-${REPLICA_INDEX} && \
                export LOG_FILE=${REPLICA_LOG_DIR}/${CURRENT_TIMESTAMP}out.log && \
                mkdir -p -m 777 ${REPLICA_LOG_DIR} && \
                mkdir -p /home/workspace &&\
                cd /home/workspace &&\
                git clone https://github.com/ynashed/bionemo-framework &&\
                cd bionemo-framework &&\
                git checkout ynashed/amplify/runai &&\
                cd sub-packages/bionemo-esm2/ &&\
                pip install -e . &&\
                cd ../../sub-packages/bionemo-amplify/ &&\
                pip install -e . &&\
                cd ../../sub-packages/bionemo-llm/ &&\
                pip install -e .  &&\
                cd ../../sub-packages/bionemo-core/ &&\
                pip install -e . &&\
                cd ../.. &&\
                ( \
                  torchrun scripts/protein/amplify/amplify_pretrain.py  \
                  --hf-dataset-name chandar-lab/UR100P   \
                  --result-dir /home/local/.cache     \
                  --experiment-name amplify350M   \
                  --wandb-project amplify_pretrain \
                  --wandb-log-model \
                  --num-gpus 8  \
                  --num-nodes 4 \
                  --random-mask-strategy all_tokens\
                  --accumulate-grad-batches 2 \
                  --micro-batch-size 64 \
                  --lr 1.0e-3 \
                  --num-layers 32 \
                  --hidden-size 960 \
                  --num-attention-heads 15 \
                  --ffn-hidden-size 3840 \
                  --max-seq-length 512 \
                  --warmup-steps 1000 \
                  --num-steps 1000000 \
                ) 2>&1 | sed "s/\r/\n/g" | tee ${LOG_FILE}
            env:
            - name: REPLICA_INDEX
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['training.kubeflow.org/replica-index']
            - name: WANDB_API_KEY
              valueFrom:
                secretKeyRef:
                  name: accesskey-wandb-key
                  key: SecretKey
            - name: TRANSFORMERS_OFFLINE
              value: '0'
            - name: TORCH_NCCL_AVOID_RECORD_STREAMS
              value: '1'
            - name: NCCL_NVLS_ENABLE
              value: '0'
            - name: NVTE_DP_AMAX_REDUCE_INTERVAL
              value: '0'
            - name: NVTE_ASYNC_AMAX_REDUCTION
              value: '1'
            - name: NVTE_FUSED_ATTN
              value: '0'
            - name: HF_HOME
              value: '/home/local/.cache'
            image: nvcr.io/nvidia/clara/bionemo-framework:2.0
            name: pytorch
            resources:
              limits:
                nvidia.com/gpu: '8'
            volumeMounts:
            - mountPath: /home/local/.cache
              name: ur100p
              readOnly: false
          volumes:
          - name: ur100p
            persistentVolumeClaim:
              claimName: ur100p-project-kht81
          schedulerName: runai-scheduler
  runPolicy:
    backoffLimit: 10