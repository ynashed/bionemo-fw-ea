import os

import pytest
import pytorch_lightning as pl
import torch
from nemo.collections.common.tokenizers.regex_tokenizer import RegExTokenizer

from bionemo.data.molecule.augment import MoleculeEnumeration, MoleculeInputTargetEnumeration
from bionemo.data.molecule.megamolbart_utils import megamolbart_build_train_valid_test_datasets, \
    megamolbart_retro_build_train_valid_test_datasets
from bionemo.model.utils import initialize_distributed_parallel_state
from tests.test_megamolbart_inference import get_cfg

os.environ["PROJECT_MOUNT"] = os.environ.get(
    "PROJECT_MOUNT", '/workspace/bionemo')

PREPEND_DIR = "../examples/tests/"
CONFIG_PATH = os.path.join(PREPEND_DIR, 'conf')
PREPEND_CONFIG_DIR = '../examples/molecule/megamolbart/conf'

TOKENIZER_MODEL = '/tokenizers/molecule/megamolbart/vocab/megamolbart.model'
TOKENIZER_VOCAB = '/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab'
SEQ_LEN = 512
SEED = 42

TOKENIZER = RegExTokenizer().load_tokenizer(regex_file=TOKENIZER_MODEL, vocab_file=TOKENIZER_VOCAB)
NUM_SAMPLES = {'train': 5, 'val': 5, 'test': 5}


@pytest.mark.needs_gpu
def test_megamolbart_build_train_valid_test_datasets():
    cfg = get_cfg(PREPEND_CONFIG_DIR, config_name='megamolbart_test', config_path=CONFIG_PATH)

    initialize_distributed_parallel_state()
    train_ds, val_ds, test_ds = megamolbart_build_train_valid_test_datasets(cfg=cfg.model.data,
                                                                            train_valid_test_num_samples=NUM_SAMPLES)
    assert len(train_ds) == NUM_SAMPLES['train'] and len(val_ds) == NUM_SAMPLES['val'] \
           and len(test_ds) == NUM_SAMPLES['test']

    assert train_ds[2] == 'Cc1ccccc1CCON'
    assert val_ds[0] == 'C#CCNC(=O)COC[C@@H](N)C#C'
    assert test_ds[4] == 'O[C@H]1COCCN(CCn2cccn2)C1'


@pytest.mark.needs_gpu
def test_megamolbart_retro_build_train_valid_test_datasets():
    cfg = get_cfg(PREPEND_CONFIG_DIR, config_name='megamolbart_downstream_retro_test', config_path=CONFIG_PATH)

    initialize_distributed_parallel_state()
    train_ds, val_ds, test_ds = megamolbart_retro_build_train_valid_test_datasets(cfg=cfg.model.data,
                                                                                  train_valid_test_num_samples=NUM_SAMPLES)
    assert len(train_ds) == NUM_SAMPLES['train'] and len(val_ds) == NUM_SAMPLES['val'] \
           and len(test_ds) == NUM_SAMPLES['test']

    input_name = cfg.model.data.input_name
    target_name = cfg.model.data.target_name
    assert input_name in train_ds[0].keys() and target_name in train_ds[0].keys()

    assert train_ds[3] == {'products': 'Cc1c(C)n(-c2cccnc2)c2ccc(Br)cc12',
                           'reactants': 'Cc1[nH]c2ccc(Br)cc2c1C.Fc1cccnc1'}

    assert val_ds[2] == {'products': 'CC(=O)NCC(=O)N(C)c1ccc(Cl)c(COc2cccc3ccc(C)nc23)c1Cl',
                         'reactants': 'CC(=O)OC(C)=O.Cc1ccc2cccc(OCc3c(Cl)ccc(N(C)C(=O)CN)c3Cl)c2n1'}


def test_molecule_enumeration_collate_fn():
    pl.seed_everything(SEED)
    collate_fn = MoleculeEnumeration(tokenizer=TOKENIZER, seq_length=SEQ_LEN,
                                     pad_size_divisible_by_8=True,
                                     encoder_mask=False, decoder_mask=False,
                                     encoder_augment=False, decoder_augment=False,
                                     canonicalize_input=False).collate_fn

    batch = ['CO[C@H](C[C@@H]1CCC[C@H]1O)c1ccccc1', 'COC[C@H](O)CCNC(=O)CCC[C@@H](C)N',
             'CC[C@](C)(Nc1cnnn1C)C1CC1']

    output = collate_fn(batch)
    expected_keys = ['text_enc', 'enc_mask', 'text_dec', 'dec_mask', 'labels', 'loss_mask', 'target_smiles']
    assert all([k in expected_keys for k in output.keys()])
    assert len(output['target_smiles']) == output['text_enc'].shape[0] and output['text_enc'].shape[0] == len(batch)
    assert output['text_enc'].shape[1] % 8 == 0

    assert not (TOKENIZER.mask_id in output['text_enc'] and TOKENIZER.mask_id in output['text_dec'])
    assert all([output['text_dec'][i][0] == TOKENIZER.bos_id for i in range(len(batch))])
    assert all([target_smi == smi for target_smi, smi in zip(output['target_smiles'], batch)])

    for i in range(len(batch)):
        mask_dec = output['dec_mask'][i]
        mask_dec[0] = 0
        dec_ids = output['text_dec'][i][mask_dec == 1].cpu().detach().numpy().tolist()

        mask_enc = output['enc_mask'][i]
        enc_ids = output['text_enc'][i][mask_enc == 1].cpu().detach().numpy().tolist()

        assert TOKENIZER.ids_to_text([dec_ids])[0] == batch[i] and \
               TOKENIZER.ids_to_text([enc_ids])[0] == batch[i]

    # double-checking masking
    collate_fn = MoleculeEnumeration(tokenizer=TOKENIZER, seq_length=SEQ_LEN,
                                     pad_size_divisible_by_8=True,
                                     encoder_mask=True, decoder_mask=True,
                                     encoder_augment=False, decoder_augment=False,
                                     canonicalize_input=False, mask_prob=0.1, span_lambda=3.0).collate_fn
    output = collate_fn(batch)
    assert (TOKENIZER.mask_id in output['text_enc']) and (TOKENIZER.mask_id in output['text_dec'])

    assert torch.equal(output['text_enc'], torch.tensor(
        [[272, 4, 287, 275, 272, 290, 274, 4, 272, 4, 281, 273, 274, 273,
          273, 273, 4, 273, 274, 0, 0, 0, 0, 0],
         [272, 285, 272, 287, 4, 279, 272, 275, 280, 285, 281, 272, 272, 272,
          290, 4, 4, 0, 0, 0, 0, 0, 0, 0],
         [272, 272, 291, 275, 272, 281, 275, 279, 273, 274, 273, 282, 282, 282,
          274, 272, 281, 272, 274, 272, 272, 274, 0, 0]]))

    assert torch.equal(output['enc_mask'], torch.tensor(
        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]))

    assert torch.equal(output['text_dec'], torch.tensor(
        [[2, 4, 4, 287, 275, 272, 290, 274, 272, 272, 272, 287, 274, 285,
          281, 273, 274, 273, 273, 273, 273, 273, 4, 0],
         [2, 4, 272, 287, 275, 4, 272, 279, 272, 4, 281, 272, 272, 272,
          290, 275, 272, 281, 279, 0, 0, 0, 0, 0],
         [2, 272, 272, 291, 275, 272, 281, 275, 279, 273, 274, 273, 282, 4,
          281, 272, 274, 4, 274, 0, 0, 0, 0, 0]]))

    assert torch.equal(output['dec_mask'], torch.tensor(
        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]))

    assert torch.equal(output['labels'], torch.tensor(
        [[4, 4, 287, 275, 272, 290, 274, 272, 272, 272, 287, 274, 285, 281,
          273, 274, 273, 273, 273, 273, 273, 4, 3, -1],
         [4, 272, 287, 275, 4, 272, 279, 272, 4, 281, 272, 272, 272, 290,
          275, 272, 281, 279, 3, -1, -1, -1, -1, -1],
         [272, 272, 291, 275, 272, 281, 275, 279, 273, 274, 273, 282, 4, 281,
          272, 274, 4, 274, 3, -1, -1, -1, -1, -1]]))

    assert torch.equal(output['loss_mask'], torch.tensor(
        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]))

    assert output['target_smiles'] == ['CO[C@H](C[C@@H]1CCC[C@H]1O)c1ccccc1', 'COC[C@H](O)CCNC(=O)CCC[C@@H](C)N',
                                       'CC[C@](C)(Nc1cnnn1C)C1CC1']


def test_molecule_input_target_enumeration_collate_fn():
    pl.seed_everything(SEED)

    input_name = 'products'
    target_name = 'reactants'
    collate_fn = MoleculeInputTargetEnumeration(tokenizer=TOKENIZER, seq_length=SEQ_LEN,
                                                pad_size_divisible_by_8=True,
                                                encoder_mask=False, decoder_mask=False,
                                                encoder_augment=False, decoder_augment=False,
                                                canonicalize_input=False,
                                                input_name=input_name,
                                                target_name=target_name).collate_fn

    batch = [{'products': 'CCc1cc(N)c(N)cc1Cl', 'reactants': 'CCc1cc([N+](=O)[O-])c(N)cc1Cl'},
             {'products': 'CC(C)(C)c1ccc(CN(CCc2ccc(Cl)c(C(F)(F)F)c2)C(=O)c2cc(Cl)cc3cc[nH]c23)cc1',
              'reactants': 'CC(C)(C)c1ccc(CNCCc2ccc(Cl)c(C(F)(F)F)c2)cc1.O=C(O)c1cc(Cl)cc2cc[nH]c12'}]

    output = collate_fn(batch)
    expected_keys = ['text_enc', 'enc_mask', 'text_dec', 'dec_mask', 'labels', 'loss_mask', 'target_smiles']
    assert all([k in expected_keys for k in output.keys()])
    assert len(output['target_smiles']) == output['text_enc'].shape[0] and output['text_enc'].shape[0] == len(batch)
    assert output['text_enc'].shape[1] % 8 == 0

    assert (TOKENIZER.mask_id not in output['text_enc']) and (TOKENIZER.mask_id not in output['text_dec'])
    assert all([output['text_dec'][i][0] == TOKENIZER.bos_id for i in range(len(batch))])

    assert all([target_smi == smi['reactants'] for target_smi, smi in zip(output['target_smiles'], batch)])

    for i in range(len(batch)):
        mask_dec = output['dec_mask'][i].clone()
        mask_dec[0] = 0
        dec_ids = output['text_dec'][i][mask_dec == 1].cpu().detach().numpy().tolist()

        mask_enc = output['enc_mask'][i]
        enc_ids = output['text_enc'][i][mask_enc == 1].cpu().detach().numpy().tolist()

        assert TOKENIZER.ids_to_text([dec_ids])[0] == batch[i]['reactants'] and \
               TOKENIZER.ids_to_text([enc_ids])[0] == batch[i]['products']

    assert torch.equal(output['text_enc'], torch.tensor(
        [[272, 272, 273, 274, 273, 273, 275, 279, 281, 273, 275, 279, 281, 273,
          273, 274, 304, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0],
         [272, 272, 275, 272, 281, 275, 272, 281, 273, 274, 273, 273, 273, 275,
          272, 279, 275, 272, 272, 273, 277, 273, 273, 273, 275, 304, 281, 273,
          275, 272, 275, 303, 281, 275, 303, 281, 303, 281, 273, 277, 281, 272,
          275, 280, 285, 281, 273, 277, 273, 273, 275, 304, 281, 273, 273, 284,
          273, 273, 286, 273, 277, 284, 281, 273, 273, 274, 0, 0, 0, 0,
          0, 0]])
                       )
    assert torch.equal(output['enc_mask'], torch.tensor(
        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]))

    assert torch.equal(output['text_dec'], torch.tensor(
        [[2, 272, 272, 273, 274, 273, 273, 275, 309, 275, 280, 285, 281, 305,
          281, 273, 275, 279, 281, 273, 273, 274, 304, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0],
         [2, 272, 272, 275, 272, 281, 275, 272, 281, 273, 274, 273, 273, 273,
          275, 272, 279, 272, 272, 273, 277, 273, 273, 273, 275, 304, 281, 273,
          275, 272, 275, 303, 281, 275, 303, 281, 303, 281, 273, 277, 281, 273,
          273, 274, 299, 285, 280, 272, 275, 285, 281, 273, 274, 273, 273, 275,
          304, 281, 273, 273, 277, 273, 273, 286, 273, 274, 277, 0, 0, 0,
          0, 0]]))

    assert torch.equal(output['dec_mask'], torch.tensor(
        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]))

    assert torch.equal(output['labels'], torch.tensor(
        [[272, 272, 273, 274, 273, 273, 275, 309, 275, 280, 285, 281, 305, 281,
          273, 275, 279, 281, 273, 273, 274, 304, 3, -1, -1, -1, -1, -1,
          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
          -1, -1],
         [272, 272, 275, 272, 281, 275, 272, 281, 273, 274, 273, 273, 273, 275,
          272, 279, 272, 272, 273, 277, 273, 273, 273, 275, 304, 281, 273, 275,
          272, 275, 303, 281, 275, 303, 281, 303, 281, 273, 277, 281, 273, 273,
          274, 299, 285, 280, 272, 275, 285, 281, 273, 274, 273, 273, 275, 304,
          281, 273, 273, 277, 273, 273, 286, 273, 274, 277, 3, -1, -1, -1,
          -1, -1]]))

    assert torch.equal(output['loss_mask'], torch.tensor(
        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]))

    assert output['target_smiles'] == ['CCc1cc([N+](=O)[O-])c(N)cc1Cl',
                                       'CC(C)(C)c1ccc(CNCCc2ccc(Cl)c(C(F)(F)F)c2)cc1.O=C(O)c1cc(Cl)cc2cc[nH]c12']


def test_megamolbart_retro_collate_fn_masking_config():
    pl.seed_everything(SEED)
    cfg = get_cfg(PREPEND_CONFIG_DIR, config_name='megamolbart_downstream_retro_test', config_path=CONFIG_PATH)
    collate_fn = MoleculeInputTargetEnumeration(tokenizer=TOKENIZER, seq_length=cfg.model.data.max_seq_length,
                                                pad_size_divisible_by_8=True,
                                                **cfg.model.data).collate_fn
    batch = [{'products': 'CCc1cc(N)c(N)cc1Cl', 'reactants': 'CCc1cc([N+](=O)[O-])c(N)cc1Cl'}]

    output = collate_fn(batch)
    assert (TOKENIZER.mask_id not in output['text_enc']) and (TOKENIZER.mask_id not in output['text_dec'])
