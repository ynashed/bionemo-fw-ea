{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffDock Model Training using BioNeMo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to provide an example use case of training a BioNeMo Geometric Diffusion model using the BioNeMo framework. At the end of this tutorial, the user will get experience in\n",
    "- configuring various config files and launch parameters for DiffDock training\n",
    "- launching single and multi-node, multi-gpu training runs\n",
    "- using NVIDIA's Base Command Platform commands for geometric diffusion model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview - DiffDock model\n",
    "\n",
    "**DiffDock** in Bionemo is based on the public DiffDock model, which is a score-based geometric diffusion model. DiffDock uses score model to perform reverse diffusion step by step to find the docking sites and the ligand poses, and use a separate confidence model to rank and select the best generated poses.\n",
    "\n",
    "This DiffDock model training example walkthrough will show how to utilize the compute resources, download and preprocess the datasets, and perform model training on single and multiple nodes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Assumptions\n",
    "\n",
    "This tutorial assumes that the user has access to BioNeMo framework and NVIDIA's BCP and DGX-Cloud compute infrastructure. The user is also expected to have required background details about \n",
    "- the BioNeMo framework, as described in the [Getting Started](../index.md) section, and \n",
    "- running the model training jobs on [BCP](../bcp-specific-commands-fw.md)\n",
    "\n",
    "All model training related commands should be executed inside the BioNeMo docker container."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting compute resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to DGX compute resource NGC site or NGC CLI\n",
    "\n",
    "As a prerequisite, configure your access to the DGX compute resources and required contents either via NVIDIA's [Base Command Platform](https://docs.nvidia.com/base-command-platform/index.html) or [NGC-CLI](https://docs.ngc.nvidia.com/cli/cmd.html) using ```ngc config set``` command. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on how to request the resources, visit [Running BioNeMo on DGX-Cloud using BCP](../../bcp-specific-commands-fw.md)\n",
    "\n",
    "**Note:**\n",
    "The interactive job launch example shown here using the Jupyter Lab interface is intended for initial user experience/trial runs. It is **strongly** advised to launch the model training jobs using the launch script as a part of the ``ngc batch run`` command, as mentioned in [Running BioNeMo on DGX-Cloud using BCP](../../bcp-specific-commands-fw.md). For DiffDock training, the model training script should be used as a template for launching the job as provided in `<BioNeMo_Workspace>/example/molecule/diffdock/scripts/train_bcp.sh`. \n",
    "\n",
    "\n",
    "\n",
    "First, let's request the resource for running the model training in an interactive manner. \n",
    "\n",
    "Here is one such example of a command for requesting the resources using NGC-CLI. Make sure to update the relevant arguments according to the compute setup, datasets, workspaces, instance types, and so on.\n",
    "\n",
    "In the configuration below, update `{deploy_ngc_org}`, `{deploy_ngc_team}` and `{deploy_container_tag}` with the correct NGC org, team name and container image tag, respectively. If there is no team name, then this can be omitted. Refer to [NGC documentation](https://docs.ngc.nvidia.com/cli/cmd_batch.html#run) for more details.\n",
    "\n",
    "\n",
    "  ```bash\n",
    "  ngc batch run \\\n",
    "    --name \"example-training-diffdock\" \\\n",
    "    --org {deploy_ngc_org} \\\n",
    "    --team {deploy_ngc_team} \\\n",
    "    --instance INSTANCE_TYPE \\            #Compute node type, such as dgxa100.80g.4.norm\n",
    "    --array-type PYTORCH \\\n",
    "    --replicas 2 \\\n",
    "    --image \"{deploy_ngc_org_team}/{deploy_container_name}:{deploy_container_tag}\" \\     #Image path for BioNeMo\n",
    "    --result /results \\\n",
    "    --workspace WORKSPACE_ID:/bionemo_diffdock:RW \\\n",
    "    --port 8888 \\\n",
    "    --datasetid DATASET_ID:/workspace/bionemo/data/ \\       # Dataset's NGC ID\n",
    "    --total-runtime 1D \\\n",
    "    --preempt RUNONCE \\                   \n",
    "    --priority MEDIUM \\                   # Priority level for the jog execution [LOW, MEDIUM, HIGH]\n",
    "    --order 1 \\                           # Priority order for the jog execution [1-99]\n",
    "    --commandline \"jupyter lab --allow-root --ip=* --port=8888 --no-browser --NotebookApp.token='' --NotebookApp.allow_origin='*' --ContentsManager.allow_hidden=True --notebook-dir=/ & sleep infinity\"  # This command can be replaced with the model training command: python train.py....\n",
    "  ```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The ``bcprun`` command provided in the cells [below](#single-node-or-multi-node-setup) can also be submitted as ``--commandline`` argument (instead of launching Jupyter-lab). \n",
    "\n",
    "Once the resources are assigned for the job and the BioNeMo container is running, we'll proceed ahead via `ngc batch exec <JOB_ID>` or using the Jupyter-Lab interface accessible at ```https://<COMPUTE_HEAD_NODE_URL_ADDRESS>:8888```. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Score Model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and pre-processing the example dataset for score model\n",
    "\n",
    "To briefly showcase the model training capacities of BioNeMo Framework, we will use a very small sample dataset (50 complex samples) from Posebusters benchmark set that is provided as a part of the preprocessed sample dataset with dataset id: 1617183\n",
    "\n",
    "If you want to do training with your own pdb data, refer to the preprocessing instructions [here](../preprocessing-bcp-training-diffdock.md). Remove the dataset id setting, update the total runtime, and update the data file directories with linking the mounted workspace folder with adding ```ln -s /bionemo_diffdock/data /workspace/bionemo/data;``` in the ```--commandline```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training: Score Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example dataset\n",
    "\n",
    "In this test runs, we will use preconfigured parameters provided in the ```train_score.yaml``` config file located in the ```/workspace/bionemo/examples/molecule/diffdock/conf``` folder.\n",
    "\n",
    "We will also set other parameters suitable for a quick run, such as ```++trainer.max_epoch=2```. User can update these parameters by editing the ``.yaml`` config file or as additional command line arguments, as shown in the example below. User can select the full dataset and adjust other parameters.\n",
    "\n",
    "As we are connected to the compute node, we navigate to the BioNeMo home folder using the command ``cd /workspace/bionemo``, and execute the following command in the terminal.\n",
    "\n",
    "The ```bcprun``` command is similar to ```srun``` command in [SLURM](https://slurm.schedmd.com/documentation.html), you can find more details at the NVIDIA [BCP User Guide](https://docs.nvidia.com/base-command-platform/user-guide/index.html). \n",
    "\n",
    "#### BCP Run commands\n",
    "\n",
    "```bash\n",
    "mkdir -p /bionemo_diffdock/results  && ln -s /bionemo_diffdock/results/ /workspace/bionemo/results\n",
    "bcprun --debug --nnodes=1 --npernode=2 \\\n",
    "    -w /workspace/bionemo \\\n",
    "    --cmd 'export PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync; \\\n",
    "    python examples/molecule/diffdock/train.py trainer.devices=2 trainer.num_nodes=1 \\\n",
    "    data.cache_path=/workspace/bionemo/data \\\n",
    "    data.split_train=/workspace/bionemo/data/splits/split_train \\\n",
    "    data.split_val=/workspace/bionemo/data/splits/split_val \\\n",
    "    data.split_test=/workspace/bionemo/data/splits/split_test \\\n",
    "    trainer.max_epochs=2 name=diffdock_score_training_test_nnodes_1_ndevices_2 \\\n",
    "    model.val_denoising_inference_freq=1 model.micro_batch_size=4 trainer.num_sanity_val_steps=0 \\\n",
    "    model.max_total_size=null model.estimate_memory_usage.maximal=null trainer.log_every_n_steps=1 '\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "To run the model training on multiple nodes, you will have to update parameters accordingly, for example, the command running the model training job on 4 nodes would look like this, but don't test this too small sample data with 4 nodes.\n",
    "```bash\n",
    "bcprun --debug --nnodes=4 --npernode=8 \\\n",
    "    -w /workspace/bionemo \\\n",
    "    --cmd 'mkdir -p /bionemo_diffdock/results ; ln -s /bionemo_diffdock/results/ /workspace/bionemo/results; export PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync; python examples/molecule/diffdock/train.py trainer.devices=8 trainer.num_nodes=4 ... '\n",
    "```\n",
    "\n",
    ":::{note}\n",
    "To run the model training job on a local workstation, user can directly execute the `train.py` script with desired configurations. For example, \n",
    "```bash\n",
    "python examples/molecule/diffdock/train.py ...\n",
    "```\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging with WandB\n",
    "\n",
    "If you are launching the model training job interactively from the terminal/Jupyter-Lab, you can set your Weights and Biases access via ```wandb login <YOUR_WANDB_API_KEY>``` or checkout https://docs.wandb.ai/ref/cli/wandb-login for more information. Alternatively, you may also export the API key as a variable at the time of launching the job via command-line, as shown in ``/workspace/bionemo/examples/molecule/scripts/train_bcp.sh``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output and Results\n",
    "\n",
    "\n",
    "As the DiffDock model training job is launched, BioNeMo will print out some of the details related to **compute resources**, **model training configuration**, and **dataset** being used for training. As the job progresses, it will also print out various details related to the test/train/validation steps and accuracy matrices at a set intervals. \n",
    "\n",
    "![diffdock_1.png](../images/diffdock_1.png)\n",
    "\n",
    "![diffdock_2.png](../images/diffdock_2.png)\n",
    "\n",
    "\n",
    "Upon the completion of training process, it will also print out the details related to log files, model checkpoints, and so on, that will also be saved in the directory as configured (here ``/workspace/bionemo/results``).\n",
    "\n",
    "![diffdock_3.png](../images/diffdock_3.png)\n",
    "\n",
    "\n",
    "Finally, if Weights and Biases logging was enabled (for example, ```++exp_manager.create_wandb_logger=True``` ), you can also visualize the model training progress and resulting matrices, and the summary also gets printed on the terminal at the end of the training job. \n",
    "\n",
    "![diffdock_4.png](../images/diffdock_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a small score model\n",
    "Adjust model with adding following config settings to the  ``python train.py``\n",
    "```bash\n",
    "model.diffusion.tr_sigma_max=34 model.ns=24 model.nv=6 model.num_conv_layers=5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Confidence Model\n",
    "Adjust the config file used for training with \n",
    "```bash\n",
    "python examples/molecule/diffdock/train.py --config-name=train_confidence ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
