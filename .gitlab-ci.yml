---
default:
  image: docker:20.10.16
  id_tokens:
    VAULT_JWT_TOKEN:
      aud: https://stg.vault.nvidia.com

       

stages:
  - code_format
  - build
  - test
  - jet
  - deploy

variables:
  # docker build options
  BIONEMO2_ONLY:
    value: "false"
    options:
      - "false"
      - "true"
    description: "a flag that determines whether to build only BFW image. Set to 'false' by default."
  DOCKER_HOST: tcp://docker:2376
  DOCKER_CERT_PATH: "$DOCKER_TLS_CERTDIR/client"
  DOCKER_TLS_VERIFY: 1
  DOCKER_TLS_CERTDIR: "/certs"
  DOCKER_BUILDKIT: 1
  BUILDKIT_PROGRESS: plain
  # image repositories
  NGC_REPO_NAME: "nvcr.io/nvidian/cvai_bnmo_trng/bionemo"
  GITLAB_REPO_NAME: "$CI_REGISTRY_IMAGE"
  IMAGE_REPO_NAME:
    value: "$CI_REGISTRY_IMAGE"
    options:
      - "$CI_REGISTRY_IMAGE"
      - "$NGC_REPO_NAME"
    description: "Allows to control whether NGC or Gitlab registry is used."
  # image tags, or used therein
  PIPELINE_TAG: "pipeline-${CI_PIPELINE_ID}"
  IMAGE_TAG: "${CI_COMMIT_SHA}"
  SUFFIX_BIONEMO2: "-bionemo2"
  SUFFIX_DEV: "-devel"
  SUFFIX_DOCS: "-docs"
  SUFFIX_QA: "-qa"
  DATE: "${CI_PIPELINE_CREATED_AT}"
  # fully-qualified image names
  IMAGE_NAME: "${IMAGE_REPO_NAME}:${IMAGE_TAG}"
  NEMO_NIGHTLY_IMAGE: "nvcr.io/nvidian/nemo-nightly:latest-nightly-main"
  NGC_CLI_FORMAT_TYPE: ascii
  NGC_CLI_ORG: nvidian
  # TODO(dorotat): change to cvai_bnmo_trng when we move data and checkpoints
  NGC_CLI_TEAM: clara-lifesciences
  # JET variables
  JET_WORKLOAD_FOLDER: "training-inference-unit-tests"
  JET_WORKLOADS_DIR: "internal/jet/workloads/$JET_WORKLOAD_FOLDER"
  ### variables excluding stages of the CI pipeline
  EXCLUDE_DOCKER_IMAGE_BUILD: "false"
  EXCLUDE_DOCS_BUILD: "false"
  EXCLUDE_PYTEST: "false"
  EXCLUDE_JET: "false"
  EXCLUDE_DEPLOY: "true"
  ### variables allowed to be modified when creating pipelines in web Gitlab GUI via Build -> Pipelines -> Run pipeline button
  ### ie when $CI_PIPELINE_SOURCE == "web"
  PYTEST:
    value: "true"
    options:
      - "false"
      - "true"
    description: "a flag that determines whether to run pytest stages. Set to 'true' by default."
  JET:
    value: "true"
    options:
      - "false"
      - "true"
    description: "a flag that determines whether to run tests in JET. Set to 'true' by default."
  NIGHTLY:
    value: "false"
    options:
      - "false"
      - "true"
    description: "a flag that determines whether to use nightly NeMo docker image as base. Set to 'false' by default."
  JET_CONV_TEST:
    value: "false"
    options:
      - "false"
      - "true"
    description: "A flag that determines whether to run partial convergence tests in CI. Set to 'false' by default."
  JET_MODEL_NAME:
    value: "all"
    options:
      - "all"
      - "OpenFold"
      - "ESM2"
      - "ESM1"
      - "ProtT5"
      - "Geneformer"
      - "DNABERT"
      - "MegaMolBART"
      - "MolMIM"
      - "DiffDock"
    description: "A flag that specifies the model for which the JET pipeline should run. Defaults to all models."
  QA_DOCKER_DEPLOY:
    value: "false"
    options:
      - "false"
      - "true"
    description: "a flag that determines whether to built & push a docker image for QA. Set to 'false' by default. Remark that only the docker image build and deploy is executed during this pipeline 
    resulting in images being pushed to both GitLab and NGC repositories, each tagged with the suffix '${CI_COMMIT_BRANCH}--${DATE_YMD}--${CI_COMMIT_SHA}-qa'"

# TODO(dorotat) solution with SKIP_CI flag is error-prone and should be replaced with better logic!!!!
workflow:
  rules:
    - if: $CI_COMMIT_BRANCH != "dev" && $QA_DOCKER_DEPLOY == "true"
      when: never
    - if: $NIGHTLY == "true" && $QA_DOCKER_DEPLOY == "true"
      when: never
    ## NOTE: QA docker image can be build (if no image on current dev) & deployed to Gitlab/NGC using CI only on dev branch.
    ## It can be built using Run pipeline page (https://gitlab-master.nvidia.com/clara-discovery/bionemo/-/pipelines)
    ## by clicking Run pipeline button and setting QA_DOCKER_DEPLOY=true
    ## Only docker build-bionemo-image stage and qa-deploy are executed. Pipeline with NIGHTLY=true cannot be executed.
    - if: $CI_COMMIT_BRANCH == "dev" && $CI_PIPELINE_SOURCE == "web" &&  $QA_DOCKER_DEPLOY == "true"
      variables:
        EXCLUDE_DOCS_BUILD: "true"
        EXCLUDE_PYTEST: "true"
        EXCLUDE_JET: "true"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /SKIP_CI/
      variables:
        EXCLUDE_DOCKER_IMAGE_BUILD: "true"
        EXCLUDE_DOCS_BUILD: "true"
        EXCLUDE_PYTEST: "true"
        EXCLUDE_JET: "true"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /PYTEST_NOT_REQUIRED/
      variables:
        EXCLUDE_PYTEST: "true"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /JET_NOT_REQUIRED/
      variables:
        EXCLUDE_JET: "true"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /BIONEMO2_ONLY/
      variables:
        BIONEMO2_ONLY: "true"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    ## NOTE: Convergence test are run using JET via CI. They can be triggered from any branch but only from the Run pipeline page
    ## (https://gitlab-master.nvidia.com/clara-discovery/bionemo/-/pipelines) by clicking Run pipeline button and setting JET_CONV_TEST=true
    ## Only docker build-bionemo-image and jet stage using training definitions from JET_WORKLOAD_FOLDER is executed (docs build and pytest stages is disabled)
    ## TODO(dorotat) Temporary disable ems2 pretrain for conv tests. enable again after ESM2 refactor is done, mid June 2024
    - if: ($CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "schedule") &&  $JET_CONV_TEST == "true"
      variables:
        EXCLUDE_DOCS_BUILD: "true"
        EXCLUDE_PYTEST: "true"
        JET_WORKLOAD_FOLDER: "partial-conv-trainings"
    - if: ($CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "schedule") && $PYTEST != "true" && $JET == "true"
      variables:
        EXCLUDE_PYTEST: "true"
        EXCLUDE_DOCS_BUILD: "true"
    - if: ($CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "schedule") &&  $PYTEST == "true" && $JET != "true"
      variables:
        EXCLUDE_JET: "true"
    - if: ($CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "schedule") &&  $PYTEST != "true" && $JET != "true"
      variables:
        EXCLUDE_PYTEST: "true"
        EXCLUDE_DOCS_BUILD: "true"
        EXCLUDE_JET: "true"
    - if: ($CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "schedule") &&  $PYTEST == "true" && $JET == "true"
    - if: $CI_COMMIT_BRANCH == "dev" && $CI_PIPELINE_SOURCE == "push"
      variables:
        EXCLUDE_DEPLOY: "false"


.docker-setup:
  services:
    - docker:20.10.16-dind
  tags:
    - dind
  before_script:
    - until docker info; do sleep 1; done
    - mkdir -p $HOME/.docker
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker login -u '$oauthtoken' -p $NGC_CLI_API_KEY nvcr.io

# Note: We need to use this docker version `docker:26.0.2-dind` or higher for buildx inline caching.
.docker-setup-bionemo2:
  services:
    - docker:26.0.2-dind
  tags:
    - dind
  before_script:
    - until docker info; do sleep 1; done
    - mkdir -p $HOME/.docker
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker login -u '$oauthtoken' -p $NGC_CLI_API_KEY nvcr.io


code_format:
  image: nvcr.io/nvidian/cvai_bnmo_trng/bionemo-linter:latest
  stage: code_format
  rules:
    - if: $JET_CONV_TEST == "true"
      when: never
    - when: always
  tags:
   - generic
  script:
    - pre-commit run --all-files --show-diff-on-failure

license_check:
  image: python:3.10.13-slim
  stage: code_format
  rules:
    - if: $JET_CONV_TEST == "true"
      when: never
    - when: always
  tags:
   - generic
  script:
    - INFRA_BIONEMO_DEP=$(cat setup/requirements-dev.txt | grep "infra-bionemo==")
    - pip install --index-url "https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/api/v4/projects/118589/packages/pypi/simple" "${INFRA_BIONEMO_DEP}"
    - license-check -c .

build-bionemo-image:
  extends:
    - .docker-setup
  stage: build
  rules:
    - if: $EXCLUDE_DOCKER_IMAGE_BUILD == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - when: on_success
  script:
    - df -h
    - if [[ $NIGHTLY = true ]]; then
      base="--build-arg BASE_IMAGE=$NEMO_NIGHTLY_IMAGE";
      else
      base="";
      fi
    # maybe we built this image on a previous pipeline run -- if so, we don't need to remake it!
    - docker pull ${IMAGE_NAME} || true
    # or maybe we built an image for the previous commit that we can use as a layer cache
    # TODO [mgreaves] There's no `git` in DIND :( Odd though -- how does it checkout the code!?
    #    - >
    #      for i in $(seq 1 10); do
    #        docker pull "${IMAGE_REPO_NAME}:$(git rev-parse HEAD~${i})" || true
    #      done
    # TODO [mgreaves] This had --no-cache on it before. Why would we want this?

    - PULL_START=$(date +%s)
    - docker pull nvcr.io/nvidian/cvai_bnmo_trng/bionemo:cache-py23.10
    - PULL_END=$(date +%s)
    - ELAPSED=$(( PULL_END - PULL_START ))
    - echo "Cache target container pull time:" $ELAPSED
    - export DOCKER_BUILDKIT=1
    - BUILD_START=$(date +%s)
    - docker buildx build
      --network host ${base}
      --label com.nvidia.bionemo.short_git_sha=${CI_COMMIT_SHORT_SHA}
      --label com.nvidia.bionemo.git_sha=${CI_COMMIT_SHA}
      --label com.nvidia.bionemo.created_at=${DATE}
      --label com.nvidia.bionemo.branch=${CI_COMMIT_BRANCH}
      --label com.nvidia.bionemo.pipeline_id=${CI_PIPELINE_ID}
      --label com.nvidia.bionemo.merge_request_id=${CI_MERGE_REQUEST_IID}
      --secret id=GITLAB_TOKEN,env=CI_JOB_TOKEN
      --cache-from nvcr.io/nvidian/cvai_bnmo_trng/bionemo:cache-py23.10
      --build-arg BUILDKIT_INLINE_CACHE=1
      -t "${IMAGE_NAME}"
      -f setup/Dockerfile .
    - BUILD_END=$(date +%s)
    - ELAPSED=$(( BUILD_END - BUILD_START ))
    - echo "container build time using cached nvcr.io/nvidian/cvai_bnmo_trng/bionemo:cache-py23.10" $ELAPSED
    - df -h
    - docker push "${IMAGE_NAME}"
    # tag with short commit too
    - docker tag "${IMAGE_NAME}" "${IMAGE_REPO_NAME}:${CI_COMMIT_SHORT_SHA}"
    - docker push "${IMAGE_REPO_NAME}:${CI_COMMIT_SHORT_SHA}"
    # Re-tag with prior pipeline ID format for backwards compatability
    - docker tag "${IMAGE_NAME}" "${IMAGE_REPO_NAME}:${PIPELINE_TAG}"
    - docker push "${IMAGE_REPO_NAME}:${PIPELINE_TAG}"
    # Also tag this image with a format that uses the date of creation:
    #       YYYY-MM-DD--CCCCCCCC
    # Where 'C..C' are the first 8 characters of the commit
    - DATE_YMD="$(echo ${DATE} | cut -f1 -d'T')"
    - TAG_DATE_SHORT_COMMIT="${CI_COMMIT_SHORT_SHA}--${DATE_YMD}"
    - docker tag "${IMAGE_NAME}" "${IMAGE_REPO_NAME}:${TAG_DATE_SHORT_COMMIT}"
    - docker push "${IMAGE_REPO_NAME}:${TAG_DATE_SHORT_COMMIT}"
    # For merge request branches only, also tag the image with the merge request number:
    #       mr--MMMM--YYYY-MM-DD--CCCCCCCC
    # Where 'M..M' is the merge request number. Note it is variable-sized.
    - TAG_MR="mr--${CI_MERGE_REQUEST_IID}--${TAG_DATE_SHORT_COMMIT}"
    - if [[ "${CI_MERGE_REQUEST_IID}" != "" ]]; then
      docker tag "${IMAGE_NAME}" "${IMAGE_REPO_NAME}:${TAG_MR}" && docker push "${IMAGE_REPO_NAME}:${TAG_MR}" && echo "MR docker tag:" "${IMAGE_REPO_NAME}:${TAG_MR}";
      fi
    #
    # FIXME [mgreaves] See note in internal/Dockerfile-devel about this (it is commented as another 'FIXME').
    #                  tl;dr Secuirty leak: passing in CI_JOB_TOKEN as a build-arg persists its value in the image!
    #
    # To fix, we'll need something like:
    #
    # - TPL_REPO=https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/dl/gwe/torch_performance_linter
    # - docker build
    #   --network host --no-cache
    #   --build-arg BIONEMO_IMAGE="$PIPELINE_TAG"
    #   --secret id=TPL_REPO,env=TPL_REPO
    #   -t ${IMAGE_NAME}${SUFFIX_DEV} -f internal/Dockerfile-devel .
    #
    # As in, ensure that the CI_JOB_TOKEN makes it way via --secret, not --build-arg.
    #
    # TODO [mgreaves] This had --no-cache on it before. Why would we want this?
    - docker build
      --network host
      --build-arg BIONEMO_IMAGE="$IMAGE_NAME"
      --build-arg TPL_REPO=https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/dl/gwe/torch_performance_linter
      --label com.nvidia.bionemo.short_git_sha=${CI_COMMIT_SHORT_SHA}
      --label com.nvidia.bionemo.git_sha=${CI_COMMIT_SHA}
      --label com.nvidia.bionemo.created_at=${DATE}
      --label com.nvidia.bionemo.branch=${CI_COMMIT_BRANCH}
      --label com.nvidia.bionemo.pipeline_id=${CI_PIPELINE_ID}
      --label com.nvidia.bionemo.merge_request_id=${CI_MERGE_REQUEST_IID}
      -t "${IMAGE_NAME}${SUFFIX_DEV}"
      -f internal/Dockerfile-devel .
    - df -h
    - docker push "${IMAGE_NAME}${SUFFIX_DEV}"
    # tag with short commit too
    - docker tag "${IMAGE_NAME}" "${IMAGE_REPO_NAME}:${CI_COMMIT_SHORT_SHA}${SUFFIX_DEV}"
    - docker push "${IMAGE_REPO_NAME}:${CI_COMMIT_SHORT_SHA}${SUFFIX_DEV}"
    # Re-tag with prior pipeline ID format for backwards compatability
    - docker tag "${IMAGE_NAME}${SUFFIX_DEV}" "${IMAGE_REPO_NAME}:${PIPELINE_TAG}${SUFFIX_DEV}"
    - docker push "${IMAGE_REPO_NAME}:${PIPELINE_TAG}${SUFFIX_DEV}"
    # Re-tag with date & short commit too
    - docker tag "${IMAGE_NAME}${SUFFIX_DEV}" "${IMAGE_REPO_NAME}:${TAG_DATE_SHORT_COMMIT}${SUFFIX_DEV}"
    - docker push "${IMAGE_REPO_NAME}:${TAG_DATE_SHORT_COMMIT}${SUFFIX_DEV}"
    - if [[ "${CI_MERGE_REQUEST_IID}" != "" ]]; then
      docker tag "${IMAGE_NAME}${SUFFIX_DEV}" "${IMAGE_REPO_NAME}:${TAG_MR}${SUFFIX_DEV}" && docker push "${IMAGE_REPO_NAME}:${TAG_MR}${SUFFIX_DEV}";
      fi

build-docs-image:
  extends:
    - .docker-setup
  rules:
    - if: $EXCLUDE_DOCS_BUILD == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - when: on_success
  stage: build
  script:
    - if [[ $NIGHTLY = true ]]; then
      base="--build-arg BASE_IMAGE=$NEMO_NIGHTLY_IMAGE";
      else
      base="";
      fi
    - cd docs/
    # CANNOT use date command -- busybox doesn't support --iso-8601
    # - DATE=$(date --iso-8601=seconds -u)
    # Good thing we have CI_PIPELINE_CREATED_AT !
    - DATE="${CI_PIPELINE_CREATED_AT}"
    - TAG_DATE="$(echo ${DATE} | cut -f1 -d'T')"
    # maybe we built this image on a previous pipeline run -- if so, we don't need to remake it!
    - docker pull ${IMAGE_NAME}${SUFFIX_DOCS} || true
    # or maybe we built an image for the previous commit(s) that we can use as a layer cache
    # TODO [mgreaves] There's no `git` in DIND :( Odd though -- how does it checkout the code!?
    #    - >
    #      for i in $(seq 1 10); do
    #        docker pull "${IMAGE_REPO_NAME}:$(git rev-parse HEAD~${i})${SUFFIX_DOCS}" || true
    #      done
    # TODO [mgreaves] This had --no-cache on it before. Why would we want this?
    - docker build
      --network host ${base}
      --label com.nvidia.bionemo.short_git_sha=${CI_COMMIT_SHORT_SHA}
      --label com.nvidia.bionemo.git_sha=${CI_COMMIT_SHA}
      --label com.nvidia.bionemo.created_at=${DATE}
      --label com.nvidia.bionemo.branch=${CI_COMMIT_BRANCH}
      --label com.nvidia.bionemo.pipeline_id=${CI_PIPELINE_ID}
      --label com.nvidia.bionemo.merge_request_id=${CI_MERGE_REQUEST_IID}
      -t "${IMAGE_NAME}${SUFFIX_DOCS}"
      -f Dockerfile.docs .
    - docker push "${IMAGE_NAME}${SUFFIX_DOCS}"
# Note: It is tempting to build the docs in the same job as build-docs-image, i.e. have a
# `docker run ... sphinx-build ...` command here, however getting the output of this build
# is non-trivial. It is non-trivial to mount volumes to docker commands when DIND is used in
# gitlab-ci, therefore we split the docs image build and docs build into separate jobs,
# where the latter doesn't require DIND.


build-docs:
  image:
    name: "${IMAGE_NAME}${SUFFIX_DOCS}"
    entrypoint: [ "" ]
  stage: build
  needs: [build-docs-image]
  tags:
    - generic
  variables:
    GIT_STRATEGY: none
    GIT_CHECKOUT: "false"
  rules:
    - if: $EXCLUDE_DOCS_BUILD == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - when: on_success
  script:
    # Build the docs, store as artifact in ${CI_PROJECT_DIR = /builds/clara-discovery/bionemo}.
    - mkdir -p /builds/clara-discovery/bionemo/docs_build
    - cd /docs
    - sphinx-build bionemo /builds/clara-discovery/bionemo/docs_build/html
  artifacts:
      paths:
        - docs_build
      expire_in: 1 week

.common_pytest_before_script: &common_before_script
  before_script:
    - nvidia-smi
    - df -Thl
    - pwd
    - echo $BIONEMO_HOME
    - ls $BIONEMO_HOME
    - mkdir -p /builds/bionemo/
    - cp -av $BIONEMO_HOME /builds/
    - rm /workspace/bionemo -rf
    - ln -s /builds/bionemo /workspace/bionemo
    - export BIONEMO_HOME=/builds/bionemo
    - echo $BIONEMO_HOME
    - cd $BIONEMO_HOME
    - ls -ltr
    - ls -lts /
    - df -Thl

pytest:
  extends:
    - .common_pytest_before_script
  image:
    name: "${IMAGE_NAME}${SUFFIX_DEV}"
    entrypoint: [ "" ]
  stage: test
  needs: [build-bionemo-image]
  rules:
    - if: $EXCLUDE_PYTEST == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - when: on_success
  tags:
    - gpu
  variables:
    GIT_STRATEGY: none
    GIT_CHECKOUT: "false"
  script:
    # TODO(trvachov): This install the NGC CLI, but this should be removed once all of our tests stop relying on data from NGC.
    # install_thirdy_party.sh: libaries needed for openfold, that cannot be distributed in the container
    - wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.40.0/files/ngccli_linux.zip -O /tmp/ngccli_linux.zip && unzip /tmp/ngccli_linux.zip -d /tmp/ && export PATH=$PATH:/tmp/ngc-cli
    - export NGC_CLI_ORG NGC_CLI_TEAM NGC_CLI_FORMAT_TYPE NGC_CLI_API_KEY
    - export AWS_ENDPOINT_URL=https://pbss.s8k.io
    - export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY
    - export MODEL_PATH=${BIONEMO_HOME}/models
    - bash ./examples/protein/openfold/scripts/install_third_party.sh
    - python download_artifacts.py --models openfold_initial_training_public openfold_finetuning_4_public openfold_initial_training_inhouse openfold_finetuning_inhouse esm2nv_3b esm2nv_8m_lora esm2nv_8m_untrained esm2nv_650m diffdock_confidence diffdock_score equidock_db5 equidock_dips megamolbart molmim_70m_24_3 prott5nv esm1nv dnabert geneformer --source pbss --model_dir $MODEL_PATH  --data all --data_dir $BIONEMO_HOME --verbose
    - unzip examples/tests/test_data/uniref202104_esm2_qc_test200_val200.zip -d examples/tests/test_data/
    - ls examples/tests/test_data/uniref202104_esm2_qc_test200_val200
    - rm -rf ./.pytest_cache/
    - mkdir -p ${CI_PROJECT_DIR}/heatmaps
    - pytest -m "not internal and not needs_fork and not needs_80gb_memory_gpu" -vv --durations=0 --cov=bionemo --cov-report term --cov-report xml:coverage.xml -k "not test_model_training" | tee pytest_report.log
    # Compute test coverage with higher precision, i.e. up to 2 decimal digits. (In the logs, we'll have 8 decimal digits.)
    # From "TOTAL ${TOTAL STATEMENTS} ${NON-COVERED STATEMENTS} ${COVERAGE PERCENT}", compute (${TOTAL STATEMENTS} - ${NON-COVERED STATEMENTS}) / ${TOTAL STATEMENTS}.
    - cat pytest_report.log | grep -Eo "TOTAL\s+[0-9]+\s+[0-9]+\s+[0-9]+%" | python -c 'import sys; lines=sys.stdin.readline(); x = lines.split(); n = int(x[1]) - int(x[2]); print(f"BIONEMO TEST COVERAGE = {100 * n / int(x[1]):.8f}%")'
    - cp -rv ${BIONEMO_HOME}/tests/data/esm2_golden_values/heatmaps ${CI_PROJECT_DIR}/heatmaps
    - cp ${BIONEMO_HOME}/coverage.xml ${CI_PROJECT_DIR}
    - df -Thl
    - if [ ! -e "${CI_PROJECT_DIR}/coverage.xml" ]; then echo "${CI_PROJECT_DIR}/coverage.xml does not exist. Failing..."; exit 1; fi
  coverage: '/BIONEMO TEST COVERAGE = ([0-9]+\.*[0-9]*%)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        # Relative path in ${CI_PROJECT_DIR}...
        path: coverage.xml
    paths:
      - heatmaps



pytest-fork:
  extends:
    - .common_pytest_before_script
  image:
    name: "${IMAGE_NAME}${SUFFIX_DEV}"
    entrypoint: [ "" ]
  stage: test
  needs: [build-bionemo-image]
  rules:
    - if: $EXCLUDE_PYTEST == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - when: on_success
  tags:
    - gpu
  variables:
    GIT_STRATEGY: none
    GIT_CHECKOUT: "false"
  script:
    - export NGC_CLI_ORG NGC_CLI_TEAM NGC_CLI_FORMAT_TYPE NGC_CLI_API_KEY
    - export AWS_ENDPOINT_URL=https://pbss.s8k.io
    - export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY
    - export MODEL_PATH=${BIONEMO_HOME}/models
    - python download_artifacts.py --models openfold_finetuning_inhouse esm2nv_3b esm2nv_650m diffdock_confidence diffdock_score equidock_db5 equidock_dips megamolbart prott5nv molmim_70m_24_3 geneformer esm1nv dnabert enformer_finetuned16 enformer_finetuned32 --source pbss --model_dir $MODEL_PATH  --data all  --data_dir $BIONEMO_HOME --verbose
    - unzip examples/tests/test_data/uniref202104_esm2_qc_test200_val200.zip -d examples/tests/test_data/
    # unit tests which need to be run in a separate process
    - bash internal/run_pytest_fork.sh
    - df -Thl


pytest-internal:
  extends:
    - .common_pytest_before_script
  image:
    name: "${IMAGE_NAME}${SUFFIX_DEV}"
    entrypoint: [ "" ]
  stage: test
  needs: [build-bionemo-image]
  allow_failure: false
  rules:
    - if: $EXCLUDE_PYTEST == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - when: on_success
  tags:
    - gpu
  variables:
    GIT_STRATEGY: none
    GIT_CHECKOUT: "false"
  script:
    - export NGC_CLI_ORG NGC_CLI_TEAM NGC_CLI_FORMAT_TYPE NGC_CLI_API_KEY
    - export AWS_ENDPOINT_URL=https://pbss.s8k.io
    - export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY
    - export MODEL_PATH=${BIONEMO_HOME}/models
    - python download_artifacts.py --models esm2_650m_huggingface esm2_3b_huggingface --source pbss --model_dir $MODEL_PATH --verbose
    - echo "Checking test data contents..."
    - rm -rf ./.pytest_cache/
    - pytest -m "internal" -v --durations=0 --cov=bionemo --cov-report term --cov-report xml:coverage.xml -k "not test_model_training"
    - df -Thl

# Build cache reference images like this:
# docker buildx build --cache-to type=inline  -t <your-image-name>  -f bionemo2/Dockerfile .
# If you build using `bash bionemo2/launch.sh build` this will be done automatically.
bionemo2-build-image:
  extends:
  # Note: We need to use this docker version `docker:26.0.2-dind` or higher for buildx inline caching.ds:
    - .docker-setup-bionemo2
  stage: build
  rules:
    - if: $BIONEMO2_ONLY == "true"
      when: always
    - if: $BIONEMO2_ONLY == "false"
      when: never
    - when: on_success
  script:
    - echo "Building bionemo2 image..."
    - cd ./bionemo2/
    - docker buildx version
    - docker buildx build
      --cache-from nvcr.io/nvidian/cvai_bnmo_trng/bionemo:bionemo2
      -t "${IMAGE_NAME}${SUFFIX_BIONEMO2}"
      -f Dockerfile .
    - echo "Now pushing container"
    - docker push "${IMAGE_NAME}${SUFFIX_BIONEMO2}"

bionemo2-pytest:
  image:
    name: "${IMAGE_NAME}${SUFFIX_BIONEMO2}"
    entrypoint: [ "" ]
  stage: test
  needs: [bionemo2-build-image]
  allow_failure: false
  rules:
    - if: $BIONEMO2_ONLY == "true"
      when: always
    - if: $BIONEMO2_ONLY == "false"
      when: never
    - if: $CI_MERGE_REQUEST_LABELS =~ /SKIP_CI/
      when: never
    - when: on_success
  tags:
    - gpu
  script:
    - nvidia-smi
    - df -Thl
    - pwd
    - ls -l
    - ln -s /builds/bionemo/bionemo2 /workspace/bionemo2
    - cd ./bionemo2/
    - echo "Running BFW tests..."
    - pwd
    - ls -l .
    - pushd ./src/bionemo-core
    - pytest -v .
    - popd
    - pushd ./src/bionemo-fw
    - pytest -v .
    - popd
    - pushd ./src/bionemo-contrib
    - pytest -v .
    - popd


.jet-configure:
  stage: jet
  tags:
    - generic
  needs:
    - job: build-bionemo-image
      optional: true
    - job: pytest
      optional: true
  after_script:
    - echo "JET_WORKLOADS_PROJECT=$CI_PROJECT_ID" >> jet.env
    - echo "JET_WORKLOADS_JOB=$CI_JOB_ID" >> jet.env
  artifacts:
    reports:
      dotenv: jet.env



jet-configure:
  extends: [.jet-configure]
  variables:
    JET_WORKLOADS_REF_BASE: "bionemo"
    JET_WORKLOADS_REF: "${JET_WORKLOADS_REF_BASE}/${CI_COMMIT_BRANCH}"
    WANDB_PROJECT_NAME_BASE: "jet--${JET_WORKLOAD_FOLDER}"
    WANDB_PROJECT_NAME: "${WANDB_PROJECT_NAME_BASE}--${CI_COMMIT_BRANCH}"
  artifacts:
    paths:
      - $JET_WORKLOADS_DIR
  rules:
    - if: $EXCLUDE_JET == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" && $EXCLUDE_JET != "true"
      when: manual
      variables:
        JET_WORKLOADS_REF: "bionemo/merge_request_event"
    - when: on_success
  before_script:
    - apk update && apk add git yq
    - 'if [[ $CI_PIPELINE_SOURCE == "merge_request_event" ]]; then
      CUSTOM_BRANCH_NAME=$(echo "$CI_MERGE_REQUEST_SOURCE_BRANCH_NAME" | tr "/" "__");
      else
      CUSTOM_BRANCH_NAME=$(echo "$CI_COMMIT_BRANCH" | tr "/" "__");
      fi'
    - export CUSTOM_BRANCH_NAME
  script:
    - yq e ".spec.source.image = \"${IMAGE_NAME}\"" -i ${JET_WORKLOADS_DIR}/builds/bionemo.yaml
    - 'find "$JET_WORKLOADS_DIR" -type f -name "*.yaml" -print0 | while IFS= read -r -d "" file; do
          echo "File: $file";
          yq e ".labels.workload_ref = \"${JET_WORKLOADS_REF}\"" -i ${file};
          yq e ".labels.bionemo_ci_pipeline_id = \"${CI_PIPELINE_ID}\"" -i ${file};
          yq e ".labels.bionemo_commit_sha = \"${CI_COMMIT_SHORT_SHA}\"" -i ${file};
          yq e ".spec.scope = \"${JET_WORKLOAD_FOLDER}\"" -i ${file};
          if [[ $JET_CONV_TEST = true ]] & [[ $file == *"recipe"* ]]; then   
            yq e ".spec.wandb_project_name = \"${WANDB_PROJECT_NAME_BASE}--${CUSTOM_BRANCH_NAME}\"" -i ${file};
            yq e ".spec.pipeline_label = \"${CI_PIPELINE_CREATED_AT}_${CI_COMMIT_SHORT_SHA}_${CI_PIPELINE_ID}\"" -i ${file};       
          fi;
          cat ${file};
       done'


.jet-trigger-template:
  stage: jet
  needs: [jet-configure]
  inherit:
    variables: true
  variables:
    JET_WORKLOADS_PROJECT: $JET_WORKLOADS_PROJECT
    JET_WORKLOADS_JOB: $JET_WORKLOADS_JOB
    JET_BUILDS_PLATFORMS: linux/amd64
  trigger:
    project: dl/jet/ci
    branch: bionemo # NOTE: this branch name enables running JET on Draco OCI
    strategy: depend

#### ADDITIONAL RESOURCES ON jet-trigger ####
# jet-trigger stage described in JET docs: https://jet.nvidia.com/docs/execution/ci-cd/downstream-pipelines/
# it uses base_config.yaml for the configuration of bionemo account under: https://gitlab-master.nvidia.com/dl/jet/ci/-/blob/bionemo/base_config.yaml?ref_type=heads
#####
# restarter - restarting jobs which failed due to <ERROR_CODE> listed under retry_on: [<ERROR_CODE>]
# for more info about error codes see https://jet.nvidia.com/docs/logs/status/
####
# deadline - increases the time limit of jobs queueing on SLURM clusters, here dgxa100_dracooci
# default deadline under corresponding section in base_config.yaml: https://gitlab-master.nvidia.com/dl/jet/ci/-/blob/bionemo/base_config.yaml?ref_type=heads#L33
jet-trigger:
  extends: [.jet-trigger-template]
  variables:
    JET_WORKLOADS_FILTER: type == 'recipe' and spec.config_name != 'pretrain_esm2_650M'
    JET_CUSTOM_CONFIG: |
      tests:
        allow_failure: true
      retrier:
        enabled: true
        max_retries: 1
        retry_on: ['1.2', '1.2.1', '1.2.1.2']
        waiting_time: 60
        environment: jet-api
      launchers:
        dgxa100_dracooci:
          additional_flags:
            deadline: now+42hours
  inherit:
    variables:
      - EXCLUDE_JET
      - JET_MODEL_NAME
      - CI_MERGE_REQUEST_LABELS
      - BIONEMO2_ONLY
  rules:
    - if: $EXCLUDE_JET == "true"
      when: never
    - if: $BIONEMO2_ONLY == "true"
      when: never
    - if: $CI_MERGE_REQUEST_LABELS =~ /OpenFold/ || $JET_MODEL_NAME == "OpenFold"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'openfold'
    - if: $CI_MERGE_REQUEST_LABELS =~ /ESM2/ || $JET_MODEL_NAME == "ESM2"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'esm2nv' and spec.config_name != 'pretrain_esm2_650M'
    - if: $CI_MERGE_REQUEST_LABELS =~ /ESM1/ || $JET_MODEL_NAME == "ESM1"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'esm1nv'
    - if: $CI_MERGE_REQUEST_LABELS =~ /ProtT5/ || $JET_MODEL_NAME == "ProtT5"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'prott5nv'
    - if: $CI_MERGE_REQUEST_LABELS =~ /Geneformer/ || $JET_MODEL_NAME == "Geneformer"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'geneformer'
    - if: $CI_MERGE_REQUEST_LABELS =~ /DNABERT/ || $JET_MODEL_NAME == "DNABERT"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'dnabert'
    - if: $CI_MERGE_REQUEST_LABELS =~ /MegaMolBART/ || $JET_MODEL_NAME == "MegaMolBART"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'megamolbart'
    - if: $CI_MERGE_REQUEST_LABELS =~ /MolMIM/ || $JET_MODEL_NAME == "MolMIM"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'molmim'
    - if: $CI_MERGE_REQUEST_LABELS =~ /DiffDock/ || $JET_MODEL_NAME == "DiffDock"
      variables:
        JET_WORKLOADS_FILTER: type == 'recipe' and spec.model == 'diffdock'
    - when: on_success


jet-test:
  rules:
    - if: $EXCLUDE_JET == "true"
      when: never
    - if: $BIONEMO2_ONLY == "false"
      needs: [jet-trigger]
      when: always
    - if: $BIONEMO2_ONLY == "true"
      when: never
  image:
    name: gitlab-master.nvidia.com:5005/dl/jet/api:latest
    entrypoint: [ "" ]
  tags:
    - generic
  stage: jet
  before_script:
    - pip install tqdm
    - export RO_API_TOKEN CI_PROJECT_ID CI_PIPELINE_ID GITLAB_USER_LOGIN
    - jet secrets jwt-login jwt/nvidia/gitlab-master bionemo-ci $VAULT_JWT_TOKEN
  script:
    - echo "CI_PROJECT_ID=${CI_PROJECT_ID}, CI_PIPELINE_ID=${CI_PIPELINE_ID}"
    - bash $CI_PROJECT_DIR/internal/jet/scripts/run_jet_test.sh


# triggering an update of the dashboard for convergence tests when the tests are finished
# Repository with the dashboard deployment: https://gitlab-master.nvidia.com/clara-discovery/dashboards
dashboard-update-trigger:
  stage: jet
  needs: [jet-trigger]
  rules:
    - if: $JET_CONV_TEST == "true" && $CI_COMMIT_BRANCH == "dev"
      when: always
  trigger:
    project: clara-discovery/dashboards
    branch: main
    strategy: depend


qa-deploy:
  stage: deploy
  extends:
    - .docker-setup
  rules:
    - if: $QA_DOCKER_DEPLOY == "true"
  script:
    - docker pull "${IMAGE_NAME}"
    - DATE_YMD="$(echo ${DATE} | cut -f1 -d'T')"
    # QA image tag has format that uses the branch, date of creation & :
    #       BBBB--YYYY-MM-DD--CCCCCCCC-qa
    # Where 'C..C' are the first 8 characters of the commit
    # Where 'B...B' is the branch name. (Note: rules filter out invalid branch names!)
    - IMAGE_TAG_QA="${CI_COMMIT_BRANCH}--${DATE_YMD}--${CI_COMMIT_SHA}${SUFFIX_QA}"
    - IMAGE_NAME_QA="${IMAGE_REPO_NAME}:${IMAGE_TAG_QA}"
    - docker tag "${IMAGE_NAME}" "${IMAGE_NAME_QA}"
    - docker push "${IMAGE_NAME_QA}"
    - docker tag "${IMAGE_NAME}" nvcr.io/nvidian/cvai_bnmo_trng/bionemo:cache-py23.10
    - docker push nvcr.io/nvidian/cvai_bnmo_trng/bionemo:cache-py23.10
    - NGC_IMAGE_NAME_QA="${NGC_REPO_NAME}:${IMAGE_TAG_QA}"
    - docker tag "${IMAGE_NAME}" "${NGC_IMAGE_NAME_QA}"
    - docker push "${NGC_IMAGE_NAME_QA}"


dev-deploy:
  extends:
    - .docker-setup
  stage: deploy
  dependencies: [ ]
  rules:
    - if: $EXCLUDE_DEPLOY == "true"
      when: never
    - when: on_success
  before_script:
    - !reference [ .docker-setup, before_script ]
    - apk update && apk add curl
  script:
    # Get the images that we need to re-tag and publish on other image registries.
    - docker pull "${IMAGE_NAME}"
    - docker pull "${IMAGE_NAME}${SUFFIX_DEV}"
    # NOTE: CANNOT use date command -- busybox doesn't support --iso-8601
    # - DATE=$(date --iso-8601=seconds -u)
    # This is why we use the gitlab CI date env var.
    #
    # Push FW image to NGC
    - docker tag "${IMAGE_NAME}" "${NGC_REPO_NAME}:${IMAGE_TAG}"
    - docker push "${NGC_REPO_NAME}:${IMAGE_TAG}"
    # Also tag this image with a format that uses the branch & date of creation*:
    #       BBBB--YYYY-MM-DD--CCCCCCCC
    # Where 'C..C' are the first 8 characters of the commit
    # Where 'B...B' is the branch name. (Note: rules filter out invalid branch names!)
    # *NOTE: the branch name is included IFF it is "main" or "dev"
    #        otherwise, the first part -- "BBBB---" -- is **NOT** included!
    - DATE_YMD="$(echo ${DATE} | cut -f1 -d'T')"
    - TAG_DATE_SHORT_COMMIT="${CI_COMMIT_SHORT_SHA}--${DATE_YMD}"
    - docker tag "${IMAGE_NAME}" "${NGC_REPO_NAME}:${TAG_DATE_SHORT_COMMIT}"
    - docker push "${NGC_REPO_NAME}:${TAG_DATE_SHORT_COMMIT}"
    # Push -devel image to NGC too
    - docker tag "${IMAGE_NAME}${SUFFIX_DEV}" "${NGC_REPO_NAME}:${CI_COMMIT_SHA}${SUFFIX_DEV}"
    - docker push "${NGC_REPO_NAME}:${CI_COMMIT_SHA}${SUFFIX_DEV}"
    # also, MAYBE tag w/ branch name and push
    - >
      if [[ "${CI_COMMIT_BRANCH}" == "main" || "${CI_COMMIT_BRANCH}" == "dev" ]]; then
        TAG_BRANCH_DATE_SHORT_COMMIT="${CI_COMMIT_BRANCH}--${TAG_DATE_SHORT_COMMIT}"
        # main image
        docker tag "${IMAGE_NAME}" "${NGC_REPO_NAME}:${TAG_BRANCH_DATE_SHORT_COMMIT}"
        docker push "${NGC_REPO_NAME}:${TAG_BRANCH_DATE_SHORT_COMMIT}"
        # -devel image
        docker tag "${IMAGE_NAME}${SUFFIX_DEV}" "${NGC_REPO_NAME}:${TAG_BRANCH_DATE_SHORT_COMMIT}${SUFFIX_DEV}"
        docker push "${NGC_REPO_NAME}:${TAG_BRANCH_DATE_SHORT_COMMIT}${SUFFIX_DEV}"
      fi

