do_training: True
do_testing: True
seed: 1204518

data:
  data_name: dips # Train first over this dataset, [db5, dips] for now only train on db5
  num_workers: 0 # Number of workers for dataloaders
  pin_memory: True
  micro_batch_size: ${model.micro_batch_size}
  world_size: ${multiply:${trainer.devices}, ${trainer.num_nodes}}
  cache_path: /data/ # Folder from where to load/restore cached dataset
  graph_cutoff: 30.0
  graph_max_neighbor: 10
  split: 0
  pocket_cutoff: 8.0
  translation_interval: 5
  n_jobs: null
  graph_residue_loc_is_alphaC: True # needed for preprocessing
  raw_data_path: /data/DIPS/data/DIPS/interim/pairs-pruned/ ## See utils/partition_dips.py on how to get this data preprocessed.
  split_files_path: /data/DIPS/data/DIPS/interim/pairs-pruned/
  reload_mode: null

model:
  name: EquiDock_${data.data_name}
  seed: 8
  restore_from_path: null # used when starting from a .nemo file
  micro_batch_size: 32
  resume_from_checkpoint: null
  debug: False
  iegmn_n_lays: 8
  graph_nodes: residues
  rot_model: kb_att
  noise_decay_rate: 0.0
  noise_initial: 0.0
  use_edge_features_in_gmn: True
  use_mean_node_features: True
  residue_emb_dim: 64
  iegmn_lay_hid_dim: 64
  input_edge_feats_dim: null
  dropout: 0.0
  nonlin: lkyrelu # ['lkyrelu', 'swish']
  cross_msgs: True
  layer_norm: LN # ['LN', 'BN', default]
  layer_norm_coors: '0'
  final_h_layer_norm: '0'
  use_dist_in_layers: True
  skip_weight_h: 0.75
  x_connection_init: 0.0
  leakyrelu_neg_slope: 0.01
  shared_layers: True
  num_att_heads: 50
  fine_tune: false
  pocket_ot_loss_weight: 1.0
  intersection_loss_weight: 10.0
  intersection_sigma: 25.0
  intersection_surface_ct: 10.0
  divide_coors_dist: false
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1

  optim:
    name: adam # fused optimizers used by model
    lr: 0.0002 # max is scaled by scheduler
    betas:
      - 0.9
      - 0.999
    eps: 1e-8
    weight_decay: 0.0001

  train_ds:
    micro_batch_size: ${model.micro_batch_size}
    pin_memory: ${data.pin_memory}
    seed: ${seed}
    cache_path: ${data.cache_path}
    num_workers: ${data.num_workers}
    shuffle: True
    data_fraction: 1.0
    drop_last: False
    data_name: ${data.data_name}
    graph_cutoff: ${data.graph_cutoff}
    graph_max_neighbor: ${data.graph_max_neighbor}
    split: ${data.split}
    pocket_cutoff: ${data.pocket_cutoff}
    translation_interval: ${data.translation_interval}
    graph_nodes: ${model.graph_nodes}
    n_jobs: ${data.n_jobs}
    graph_residue_loc_is_alphaC: ${data.graph_residue_loc_is_alphaC} # needed for preprocessing
    raw_data_path: ${data.raw_data_path}
    split_files_path: ${data.split_files_path}
    reload_mode: 'train'

  validation_ds:
    micro_batch_size: ${model.micro_batch_size}
    pin_memory: ${data.pin_memory}
    seed: ${seed}
    cache_path: ${data.cache_path}
    num_workers: ${data.num_workers}
    shuffle: False
    data_fraction: null
    drop_last: False
    data_name: ${data.data_name}
    graph_cutoff: ${data.graph_cutoff}
    graph_max_neighbor: ${data.graph_max_neighbor}
    split: ${data.split}
    pocket_cutoff: ${data.pocket_cutoff}
    translation_interval: ${data.translation_interval}
    graph_nodes: ${model.graph_nodes}
    n_jobs: ${data.n_jobs}
    graph_residue_loc_is_alphaC: ${data.graph_residue_loc_is_alphaC}
    raw_data_path: ${data.raw_data_path}
    split_files_path: ${data.split_files_path}
    reload_mode: 'val'

  test_ds:
    micro_batch_size: ${model.micro_batch_size}
    pin_memory: ${data.pin_memory}
    seed: ${seed}
    cache_path: ${data.cache_path}
    num_workers: ${data.num_workers}
    shuffle: False
    data_fraction: null
    drop_last: False
    data_name: ${data.data_name}
    graph_cutoff: ${data.graph_cutoff}
    graph_max_neighbor: ${data.graph_max_neighbor}
    split: ${data.split}
    pocket_cutoff: ${data.pocket_cutoff}
    translation_interval: ${data.translation_interval}
    graph_nodes: ${model.graph_nodes}
    n_jobs: ${data.n_jobs}
    graph_residue_loc_is_alphaC: ${data.graph_residue_loc_is_alphaC}
    raw_data_path: ${data.raw_data_path}
    split_files_path: ${data.split_files_path}
    reload_mode: 'test'

trainer:
  devices: 2
  num_nodes: 1
  precision: 32 # To activate AMP set to 16 or 'bf16'; otherwise will be float32
  accelerator: gpu # gpu or cpu
  max_epochs: 1000 # set to null when using max_steps instead with NeMo model
  max_steps: -1
  log_every_n_steps: 1 # number of iterations between logging
  val_check_interval: 1.0 # set to integer when using steps to determine frequency of validation, use fraction with epochs
  num_sanity_val_steps: 1.0 # set to 0 or small number to test validation before training
  limit_val_batches: 1.0 # number of batches in validation step, use fraction for fraction of data
  limit_test_batches: 1.0 # number of batches in test step, use fraction for fraction of data
  limit_train_batches: 1.0
  gradient_clip_val: 100.0
  logger: False # logger is provided by NeMo exp_manager
  enable_checkpointing: False # checkpointing is done by NeMo exp_manager
  reload_dataloaders_every_n_epochs: 0 # Set to a non-negative integer to reload dataloaders every n epochs. Default: ``0``.
  accumulate_grad_batches: 1
  

exp_manager:
  name: ${model.name}_nnodes_${trainer.num_nodes}_ndevices_${trainer.devices}_bs_${data.micro_batch_size}
  # checkpoint reloading and saving
  resume_if_exists: True # autmatically resume if checkpoint exists
  resume_ignore_no_checkpoint: True # leave as True, will start new training if resume_if_exists is True but no checkpoint exists
  create_checkpoint_callback: True # leave as True, use exp_manger for checkpoints

  checkpoint_callback_params:
    save_top_k: 1 # number of checkpoints to save
    monitor: val_complex_rmsd_median # use loss to select best checkpoints
    mode: min # use min or max of monitored metric to select best checkpoints
    save_last: True # always save last checkpoint
    always_save_nemo: True # not implemented for model parallel, additionally save NeMo-style checkpoint during validation, set to False if checkpoint saving is time consuming
    filename: '${model.name}--{val_complex_rmsd_median:.4f}-{step}' # -{consumed_samples}'
    save_best_model: True
    save_nemo_on_train_end: True
  
  # ********************************************************* #
  # Early stopping callback works with the latest nemo
  create_early_stopping_callback: True
  early_stopping_callback_params:
    monitor: val_complex_rmsd_median
    mode: 'min' 
    patience: 500
    verbose: True
    check_on_train_epoch_end: False
  
  #EMA
  ema: # Exponential Moving Average; is picked up by exp_manager()
    enable: False # Creates EMA callback in exp_manager
    decay: 0.999 # (ema_rate) The exponential decay used when calculating the moving average. Has to be between 0-1.
    
  # logging
  exp_dir: /result/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}
  explicit_log_dir: ${.exp_dir}
  create_tensorboard_logger: True
  create_wandb_logger: True
  wandb_logger_kwargs:
    project: ${model.name}_training
    name: lr_${model.optim.lr}_iegmn_n_lays_${model.iegmn_n_lays}_iegmn_hdim_${model.iegmn_lay_hid_dim}
    group: ${model.name}
    job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}
    notes: "date: ${now:%y%m%d-%H%M%S}"
    tags:
      -${model.name}
      -${data.data_name}
    offline: True # set to True if there are issues uploading to WandB during training
