# This base config performs the training task where inputs molecules are randomly rotated to a non-canonical form and decoder outputs
#  match. This is roughly going to learn an equivarant model to this form of augmentation.

defaults:
  - .@model.encoder: megatron_model_base_config
  - .@model.decoder: megatron_model_base_config

name: MolMIM
do_training: True # set to false if data preprocessing steps must be completed
do_testing: False # set to true to run evaluation on test data after training, requires test_dataset section
seed: 42
restore_from_path: null # used when starting from a .nemo file

trainer:
  devices: 8 # number of GPUs or CPUs
  num_nodes: 8
  precision: bf16-mixed # 16-mixed, 32
  accelerator: gpu # gpu, cpu
  max_epochs: null # use max_steps instead with NeMo Megatron model
  max_steps: 425000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  log_every_n_steps: 100 # number of interations between logging
  val_check_interval: 10000 # set to integer when using steps to determine frequency of validation, use fraction with epochs
  num_sanity_val_steps: 2 # set to 0 or small number to test validation before training
  limit_train_batches: 1. # number of batches in train step, use fraction for fraction of data
  limit_val_batches: 200 # number of batches in validation step, use fraction for fraction of data, 0 to disable
  limit_test_batches: 0 # number of batches in test step, use fraction for fraction of data, 0 to disable
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  logger: False # logger is provided by NeMo exp_manager
  enable_checkpointing: False # checkpointing is done by NeMo exp_manager
  use_distributed_sampler: False # use NeMo Megatron samplers

model:
  name: ${name}-united-token_head
  # model parallelism
  micro_batch_size: 32
  global_batch_size: null
  tensor_model_parallel_size: 1 # model parallelism
  pipeline_model_parallel_size: 1
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  pipeline_model_parallel_split_rank: 0 # rank at which decoder starts.

  # model architecture
  make_vocab_size_divisible_by: 128 # Pad the vocab size to be divisible by this value for computation efficiency.
  pre_process: True # add embedding
  post_process: True # add pooler
  megatron_amp_O2: False # use AMP with O2 style mixed precision instead of native amp on-the-fly weight autocasting.
  seq_length: 128 # maximum sequence length allowed, note that 128 was longer than all 1.8B molecules in the zinc drug-like DB. This will speed up generation.
  max_position_embeddings: ${.seq_length} # maximum sequence length allowed
  gradient_as_bucket_view: True # Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  bias_gelu_fusion: True # Use a kernel that fuses the bias addition from weight matrices with the subsequent gelu activation.
  share_token_embeddings: True # If True share encoder/decoder embeddings
  share_decoder_tokens_head_embeddings: False # If True share decoder embeddings and decoder projection to logits
  hidden_size: ${model.encoder.hidden_size}

  # make encoder and decoder the same
  encoder:
    arch: perceiver # change encoder architecture to perceiver
    # The `hidden_steps` argument controls the K value, how many latent embeddings are output by the Perceiver encoder
    #   in the paper and Service for property guided generation we typically use K=1, however the default value if not set is K=32.
    #   (see `nemo.collections.nlp.modules.common.megatron.megatron_perceiver_encoders.MegatronPerceiverEncoderModule`)
    hidden_steps: 1
    num_layers: 3
    hidden_size: 512  # This controls the D value, the dimension of our latent embeddings.
    num_attention_heads: 12
    ffn_hidden_size: ${multiply:${.hidden_size}, 4} # Transformer FFN hidden size. Usually 4 * hidden_size.
  decoder:
    arch: transformer
    num_layers: ${model.encoder.num_layers}
    hidden_size: ${model.encoder.hidden_size}
    num_attention_heads: ${model.encoder.num_attention_heads}
    ffn_hidden_size: ${model.encoder.ffn_hidden_size}
  # define the hiddens transformation for MolMIM
  training_callbacks:
    - class: bionemo.callbacks.scheduling_callbacks.ParameterMultiplicativeScheduler
      factor: 0.001  # linear increase over first 1k steps
      min_multiplier: 0.0
      max_multiplier: 1.0
      # the path to the parameter to be scheduled. Note .0. works for getattr on items in a MoluleList
      module_parameter_path: "enc_dec_model.enc_dec_model.hiddens_module.hidden_loss_transforms.0.loss_weight"
  hiddens:
    enc_output_name: z  # The latent output to use as input to the decoder during training
    enc_inference_output_name: z_mean  # the latent output to use as input to the decoder during inference
    token_aggregation_method: mean  # how to aggregate token-level reconstruction loss. "mean" puts equal weight per sample in a batch regardless of length.
    hidden_aggregation_method: mean # how to aggregate hidden-level reconstruction loss. "mean" will scale this loss by the length of z, so higher dimensional models do not need to be scaled down.
    transform:
      q_z_given_x:
        # #  The baseline MIM loss configuration is a conditional Gaussian with a minimum variance of 1e-6.
        # cls_name: cond_gaussian
        # hidden_size: ${model.encoder.hidden_size}
        # min_logvar: -6.0
        # # The following config block is an example of "log variance sampling"
        cls_name: sampled_var_cond_gaussian
        min_logvar: -6.0
        max_logvar: 0.0   # variance is sampled from uniform distribution between min and max in log space
        map_var_to_hiddens: False  # If True, z_mean is a function (partially) of the sampled log variance. Also try setting to False.
        hidden_size: ${model.encoder.hidden_size}
        # # The following config block is an example of "log variance interpolation" between the model estimate and a unit gaussian
        # cls_name: interp_var_cond_gaussian
        # min_logvar: -6.0
        # map_coef_to_hiddens: True  # If True, z_mean is a function (partially) of the sampled log variance. Also try setting to False.
        # hidden_size: ${model.encoder.hidden_size}
    loss:
      mim:
        cls_name: a_mim
        loss_weight: 1.0  # since we use "mean" aggregation for both token/hidden, weight of 1 here is ok.

  tokenizer:
    library: 'regex'
    type: null
    model: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/molmim/vocab/molmim.model
    vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/molmim/vocab/molmim.vocab
    merge_file: null

  data:
    links_file: ${oc.env:BIONEMO_HOME}/examples/molecule/megamolbart/dataset/ZINC-downloader.txt # Set to ZINC-downloader-sample.txt for demo
    dataset_path: /data/zinc_csv # parent directory for data, contains train / val / test folders. Needs to be writeable for index creation.
    dataset: # inclusive range of data files to load or can load a single file, e.g. x000 or x[000..186]
      train: x[000..186] # Range is for full dataset
      test: x[000..186]
      val: x[000..186]
    # The following should probably always be True, unless you want to explore the unchanged input data. 
    #  Changing will impact val/test metrics since canonicalization is done to model outputs but not 
    #  targets (assuming it is done here).
    canonicalize_target_smile: True
    # The following four settings will result in randomized augmentation of SMILES for input, and matched output.
    #  this is the equivarant model case.
    canonicalize_encoder_input: False
    canonicalize_decoder_output: False
    encoder_augment: True
    decoder_independent_augment: False
    encoder_mask: False # task = mask(span)_aug
    decoder_mask: False # always False
    mask_prob: 0.0 # mask probability
    span_lambda: 3.0 # max size of poisson distribution for 
    micro_batch_size: ${model.micro_batch_size}
    num_workers: 10
    dataloader_type: single
    max_seq_length: ${model.seq_length}
    seed: ${seed}
    skip_lines: 0
    drop_last: False
    pin_memory: False
    data_impl: "csv_mmap"
    index_mapping_type: online
    data_impl_kwargs:
      csv_mmap:
        newline_int: 10 # byte-value of newline
        header_lines: 1 # skip first N header lines
        workers: ${model.data.num_workers} # number of workers when creating missing index files (null defaults to cpu_num // 2)
        sort_dataset_paths: True # if True datasets will be sorted by name
        data_sep: ',' # string to split text into columns
        # column number of csv to take values from
        data_col: 1
    use_upsampling: False # if the data should be upsampled to max number of steps in the training

  optim:
    name: fused_adam
    lr: 5e-4
    weight_decay: 0.001  # make this the same order as max lr
    betas:
      - 0.9
      - 0.999  # 0.95 seems more robust to large LR+large batch than 0.999, but this could be a one-off
    sched:
      name: CosineAnnealing
      warmup_steps: ${multiply:${trainer.max_steps}, 0.01}  # TODO make this related to trainer.max_steps
      constant_steps: ${multiply:${trainer.max_steps}, 0.05}  # hold at the minimum learning rate for the last 5% of steps
      max_steps: ${trainer.max_steps}
      min_lr: 5e-5

  dwnstr_task_validation:
    enabled: False
    dataset:
      class: bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback
      task_type: regression
      infer_target: bionemo.model.molecule.molmim.infer.MolMIMInference
      max_seq_length: ${model.seq_length}
      emb_batch_size: 128
      batch_size: 128
      num_epochs: 10
      shuffle: True
      num_workers: 8
      dataset_path: /data/physchem/
      task_name: SAMPL
      dataset:
        train: x000
        test: x000
      sequence_column: 'smiles'
      target_column: 'expt'
      random_seed: 1234
      optim:
        name: adam
        lr: 0.0001
        betas:
          - 0.9
          - 0.999
        eps: 1e-8
        weight_decay: 0.01
        sched:
          name: WarmupAnnealing
          min_lr: 0.00001
          last_epoch: -1
          warmup_ratio: 0.01
          max_steps: 1000

exp_manager:
  name: ${name}

  # checkpoint reloading and saving
  resume_if_exists: True # autmatically resume if checkpoint exists
  resume_ignore_no_checkpoint: True # leave as True, will start new training if resume_if_exists is True but no checkpoint exists
  create_checkpoint_callback: True # leave as True, use exp_manger for for checkpoints
  checkpoint_callback_params:
    save_top_k: 3 # number of checkpoints to save
    monitor: val_molecular_accuracy # use molecular accuracy to select best checkpoints
    mode: max # use min or max of monitored metric to select best checkpoints
    save_last: True # always save last checkpoint
    always_save_nemo: True # not implemented for model parallel, additionally save NeMo-style checkpoint during validation, set to False if checkpoint saving is time consuming
    filename: '${name}-${model.name}--{val_molecular_accuracy:.2f}-{val_loss:.2f}-{step}-{consumed_samples}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}

  # logging
  exp_dir: /result/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}
  explicit_log_dir: ${.exp_dir}
  create_wandb_logger: True
  create_tensorboard_logger: True
  wandb_logger_kwargs:
    project: ${model.name}_pretraining
    name: ${model.name}_pretraining
    group: ${model.name}
    job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}
    notes: "date: ${now:%y%m%d-%H%M%S}"
    tags:
      - ${name}
      - ${model.name}
    offline: False # set to True if there are issues uploading to WandB during training
