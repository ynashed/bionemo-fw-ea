defaults:
  - dnabert_base_config

# 2 devices 
trainer:
  devices: 2
  val_check_interval: 1000
  # Custom settings to reduce time spent in validation
  log_every_n_steps: 100
  limit_val_batches: 8 # number of batches in validation step, use fraction for fraction of data, 0 to disable
  limit_test_batches: 500 # number of batches in test step, use fraction for fraction of data, 0 to disable
  max_steps: 400000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
model:
  num_layers: 6
  micro_batch_size: 128
  hidden_size: 256
  ffn_hidden_size: 2048 # Transformer FFN hidden size. Usually 4 * hidden_size.
  num_attention_heads: 4
  pre_process: True # add embedding
  post_process: True # add pooler
  bert_binary_head: False # BERT binary head
  tokenizer:
    type: 'kmer'
    k: 3
    model: ${oc.env:BIONEMO_HOME}/tokenizers/dna/dnabert/vocab/dnabert${.k}.model
    vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/dna/dnabert/vocab/dnabert${.k}.vocab
  data:
    dataset_path: ${oc.env:BIONEMO_HOME}/data/GRCh38.p13 # contains train / val / test folders
    dataset: # inclusive range of data files to load or can load a single file, e.g. x000.csv
      train: chr[1..19].fna.gz.chunked.fa
      test: chr21.fna.gz.chunked.fa
      val: chr20.fna.gz.chunked.fa
    micro_batch_size: ${model.micro_batch_size}
    num_workers: 24
    dataloader_type: single
    dataset_format: fasta

  optim:
    name: fused_adam
    lr: 2e-3
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 500
      constant_steps: 50000
      min_lr: 1e-5

exp_manager:
  explicit_log_dir: null
  exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}
  name: dnabert
  create_wandb_logger: True
  wandb_logger_kwargs:
    project: null
    name: dnabert-xsmall
    offline: False
  resume_if_exists: False
