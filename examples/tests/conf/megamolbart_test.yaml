defaults:
  - pretrain_xsmall_span_aug

trainer:
  devices: 1
  num_nodes: 1
  max_epochs: 1 # PTL default. In practice we don't usually train for more than 1 epoch.
  max_steps: 500 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  val_check_interval: 50
  limit_val_batches: 1
  accumulate_grad_batches: 1
    
model:
  micro_batch_size: 4
  global_batch_size: null
  data:
    dataset_path: ${oc.env:PROJECT_MOUNT}/examples/tests/test_data/molecule/ # parent directory for data, contains train / val / test folders
    dataset: # inclusive range of data files to load or can load a single file, e.g. x000.csv
      train: x[000..001]
      test: x[000..001]
      val: x[000..001]
    micro_batch_size: ${model.micro_batch_size}
    data_col: 1 # 0-based
    data_sep: ','
    header_lines: 1
    num_workers: 10

exp_manager:
  exp_dir: /tmp/nemo_experiments/megamolbart_pretrain
  create_wandb_logger: False
  create_tensorboard_logger: False
  wandb_logger_kwargs:
    offline: True